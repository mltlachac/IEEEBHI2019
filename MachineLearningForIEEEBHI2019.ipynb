{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#author: Ermal Toto\n",
    "#edited by ML Tlachac\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import collections\n",
    "import operator\n",
    "import argparse\n",
    "import random\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn import svm\n",
    "from statistics import mean \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.utils import resample\n",
    "from sklearn import preprocessing\n",
    "from sklearn import utils\n",
    "from sklearn.datasets import load_digits\n",
    "from sklearn.feature_selection import SelectKBest, chi2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#generate 100 random numbers as each model is run 100 times with different random seeds\n",
    "\n",
    "rlist = []\n",
    "count = 0\n",
    "while count<100:\n",
    "    rlist.append(random.randint(0,1000000000))\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "resultsDF = pd.DataFrame()\n",
    "fileList = []\n",
    "modelList = []\n",
    "resampleList = []\n",
    "nFeaturesList = []\n",
    "phq9cutList = []\n",
    "kernelNeighborList = []\n",
    "precisionList = []\n",
    "recallList = []\n",
    "f1List = []\n",
    "accuracyList = []\n",
    "truePosList = []\n",
    "trueNegList = []\n",
    "falsePosList = []\n",
    "falseNegList = []\n",
    "randoms = []\n",
    "\n",
    "verbose = False\n",
    "dataStart = 3\n",
    "dataEnd = 51\n",
    "targetData = -1 #last feature used as class variable. \n",
    "targetDataCount = 1\n",
    "folds = 5\n",
    "scoring = ['precision', 'recall', 'f1','accuracy']\n",
    "scoref = ['f1']\n",
    "missingValues = -999 #Remove Instances: Remove, 0, -100, -1\n",
    "doFeatureSelection = \"False\"\n",
    "printResultHeader = \"True\"\n",
    "parser = argparse.ArgumentParser()\n",
    "modelType = \"kNN\" #SVC, RF, kNN, XG, LR, NB, ADA\n",
    "numNeighborsList = [5]\n",
    "svckernelList = [\"poly\"]\n",
    "cutoff = 10 #phq-9 cutoff for prediction\n",
    "resampleType = \"down\" #if anything else, no data balancing occurs\n",
    "numberOfFeaturesList = [10] #if doing feature selection\n",
    "doFeatureSelection = \"False\"\n",
    "datasets = [\"nTBdf.csv\", \"tTBdf.csv\"]\n",
    "newScores = []\n",
    "\n",
    "for number in rlist:\n",
    "    r = number\n",
    "    random.seed(r)\n",
    "    for d in datasets:\n",
    "        filename = d\n",
    "        for f in numberOfFeaturesList:\n",
    "            numberOfFeatures = f\n",
    "            for i in range(0,1): #change range for methods with more values in numNeighborsList\n",
    "\n",
    "                if numberOfFeatures > (dataEnd - dataStart):\n",
    "                        numberOfFeatures = dataEnd - dataStart\n",
    "\n",
    "                data = pd.read_csv(filename)\n",
    "\n",
    "                #remove unwanted column if needed\n",
    "                if \"n_nChar\" in set(data.columns):\n",
    "                    data = data.drop(columns = [\"n_nChar\"]).reset_index()\n",
    "                elif \"t_nChar\" in set(data.columns):\n",
    "                    data = data.drop(columns = [\"t_nChar\"]).reset_index()\n",
    "                else:\n",
    "                    data = data.drop(columns = [\"k_nChar\"]).reset_index()\n",
    "\n",
    "                featureSubset = data[data.columns[dataStart:dataEnd]] #Skip PHQ-9 Responses \n",
    "                print(featureSubset.columns)\n",
    "                target = data[data.columns[targetData]]\n",
    "                print(target.head())\n",
    "                featureSubset=featureSubset.assign(target = target)\n",
    "                if missingValues == '-999':\n",
    "                    featureSubset = featureSubset.dropna()\n",
    "                else:\n",
    "                    featureSubset = featureSubset.replace(np.nan, missingValues, regex=True) # Replace all missing values with missingValues as defined\n",
    "                featureSubset[featureSubset.columns[-1]] = np.where(featureSubset[featureSubset.columns[-1]] > cutoff, 1, 0)\n",
    "\n",
    "                #Identify majority and miniority class for downsample proceedures\n",
    "                targetClassCount = collections.Counter(featureSubset[featureSubset.columns[-1]])\n",
    "                majorityKey = max(targetClassCount, key=targetClassCount.get)\n",
    "                majorityCount = targetClassCount[majorityKey]\n",
    "                minorityKey = min(targetClassCount,  key=targetClassCount.get)\n",
    "                minorityCount = targetClassCount[minorityKey]\n",
    "\n",
    "                #Separate minority and majority classes\n",
    "                featureSubset_majority = featureSubset[featureSubset[featureSubset.columns[len(featureSubset.columns)-1]] == majorityKey]\n",
    "                featureSubset_minority = featureSubset[featureSubset[featureSubset.columns[len(featureSubset.columns)-1]] == minorityKey]\n",
    "\n",
    "                # Downsample minority class\n",
    "                featureSubset_majority_downsampled = resample(featureSubset_majority, \n",
    "                                                 replace=False,     # sample with replacement\n",
    "                                                 n_samples=minorityCount,    # to match majority class\n",
    "                                                 random_state=r) # reproducible results\n",
    "\n",
    "                # Combine majority class with upsampled minority class\n",
    "                featureSubset_downsampled = pd.concat([featureSubset_majority_downsampled, featureSubset_minority])\n",
    "                featureSubset_downsampled = featureSubset_downsampled.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "                if resampleType == \"down\":\n",
    "                    featureSubset = featureSubset_downsampled\n",
    "\n",
    "\n",
    "                # separate target from features\n",
    "                target = featureSubset[featureSubset.columns[-1]]  \n",
    "                featureSubset = featureSubset[featureSubset.columns[:targetDataCount*-1]] #Skip PHQ-9 Responses \n",
    "\n",
    "\n",
    "                # In[25]: Scale date between 0 and 1. Several algorithms including feature selection need this. \n",
    "                min_max_scaler = preprocessing.MinMaxScaler()\n",
    "                np_scaled = min_max_scaler.fit_transform(featureSubset)\n",
    "                featureSubset = pd.DataFrame(np_scaled)\n",
    "\n",
    "\n",
    "\n",
    "                # In[26]: Feature Selection\n",
    "                if(doFeatureSelection == \"True\"):\n",
    "                    from sklearn.datasets import load_digits\n",
    "                    from sklearn.feature_selection import SelectKBest, chi2\n",
    "                    featureSubset = SelectKBest(chi2, k=numberOfFeatures).fit_transform(featureSubset, target)\n",
    "\n",
    "                #SVC, RF, kNN, XG\n",
    "                if modelType == \"SVC\":\n",
    "                    svckernel = svckernelList[i]\n",
    "                    kernelNeighborList.append(svckernelList[i])\n",
    "                    clf = svm.SVC(kernel=svckernel, C=1, random_state=0)\n",
    "                elif modelType == \"RF\":\n",
    "                    kernelNeighborList.append(i)\n",
    "                    clf = RandomForestClassifier(n_estimators=100, max_depth=i,random_state=0)\n",
    "                elif modelType == \"kNN\":\n",
    "                    numNeighbors = numNeighborsList[i]\n",
    "                    kernelNeighborList.append(numNeighborsList[i])\n",
    "                    clf = KNeighborsClassifier(n_neighbors=numNeighbors)\n",
    "                elif modelType == \"XG\":\n",
    "                    kernelNeighborList.append(i)\n",
    "                    clf = xgb.XGBClassifier(max_depth=i)\n",
    "                elif modelType == \"LR\":\n",
    "                    kernelNeighborList.append(\"NA\")\n",
    "                    clf = LogisticRegression(random_state=r)\n",
    "                elif modelType == \"NB\":\n",
    "                    kernelNeighborList.append(\"NA\")\n",
    "                    clf = GaussianNB()\n",
    "                elif modelType == \"ADA\":\n",
    "                    kernelNeighborList.append(\"NA\")\n",
    "                    clf = AdaBoostClassifier(n_estimators=100, random_state=r)\n",
    "\n",
    "                scores = cross_validate(clf, featureSubset, target, scoring=scoring,cv=folds, return_train_score=False)\n",
    "                y_pred = cross_val_predict(clf, featureSubset, target, cv=folds)\n",
    "\n",
    "                conf_mat = confusion_matrix(target, y_pred)\n",
    "                TP = conf_mat[0][0]\n",
    "                TN = conf_mat[1][1]\n",
    "                FP = conf_mat[0][1]\n",
    "                FN = conf_mat[1][0]\n",
    "                precision = mean(scores['test_precision'])\n",
    "                sensitivity = mean(scores['test_recall'])\n",
    "                f1 = mean(scores['test_f1'])\n",
    "                accuracy = mean(scores['test_accuracy'])\n",
    "\n",
    "                randoms.append(r)\n",
    "                fileList.append(d)\n",
    "                modelList.append(modelType)\n",
    "                resampleList.append(s)\n",
    "                nFeaturesList.append(f)\n",
    "                phq9cutList.append(c)\n",
    "                precisionList.append(precision)\n",
    "                recallList.append(sensitivity)\n",
    "                f1List.append(f1)\n",
    "                accuracyList.append(accuracy)\n",
    "                truePosList.append(TP)\n",
    "                trueNegList.append(TN)\n",
    "                falsePosList.append(FP)\n",
    "                falseNegList.append(FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "resultsDF[\"file\"] = fileList\n",
    "resultsDF[\"model\"] = modelList\n",
    "resultsDF[\"resample\"] = resampleList\n",
    "resultsDF[\"nFeatures\"] = nFeaturesList\n",
    "resultsDF[\"phq9cut\"] = phq9cutList\n",
    "resultsDF[\"kernelNeighbor\"] = kernelNeighborList\n",
    "resultsDF[\"precision\"] = precisionList\n",
    "resultsDF[\"recall\"] = recallList\n",
    "resultsDF[\"f1List\"] = f1List\n",
    "resultsDF[\"accuracy\"] = accuracyList\n",
    "resultsDF[\"truePos\"] = truePosList\n",
    "resultsDF[\"trueNeg\"] = trueNegList\n",
    "resultsDF[\"falsePos\"] = falsePosList\n",
    "resultsDF[\"falseNeg\"] = falseNegList\n",
    "resultsDF[\"r\"] = randoms\n",
    "resultsDF.to_csv(\"resultsDF.csv\")"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
